# Elsie Database Populator & Management System

This directory contains tools for managing the Elsie database, including the modular wiki crawler and backup/restore functionality.

## ğŸš€ Quick Start

### Wiki Crawling
```bash
# Standard crawl (~26 curated pages)
python wiki_crawler.py

# Crawl specific page
python wiki_crawler.py "USS Stardancer"

# Show database statistics
python wiki_crawler.py --stats
```

### Database Management
```bash
# Create initial seed after database population
python backup_db.py seed

# Restore from seed backup
python backup_db.py restore
```

## ğŸ“ Modular Architecture

The wiki crawler has been refactored into a clean modular architecture:

### Core Components

| Module | Lines | Purpose |
|--------|-------|---------|
| **ğŸ¯ `wiki_crawler.py`** | 253 | Main orchestrator and entry point |
| **ğŸ“¡ `api_client.py`** | 253 | MediaWiki API operations |
| **ğŸ”„ `content_processor.py`** | 414 | Content formatting & classification |
| **ğŸš€ `content_extractor.py`** | 221 | High-level extraction strategies |
| **ğŸ’¾ `db_operations.py`** | 266 | Database operations |

### Module Responsibilities

**ğŸ¯ Main Orchestrator (`wiki_crawler.py`)**
- Coordinates all modular components
- Handles command-line arguments and main crawling loop
- **83% size reduction** from original monolithic design

**ğŸ“¡ API Client (`api_client.py`)**
- `MediaWikiAPIClient` class with all API methods
- Combined API calls for performance optimization
- Retry logic and error handling

**ğŸ”„ Content Processor (`content_processor.py`)**
- `ContentProcessor` class with ship classification
- Text processing and content formatting
- Hash calculation and content validation

**ğŸš€ Content Extractor (`content_extractor.py`)**
- `ContentExtractor` class orchestrating extraction
- Multiple fallback strategies (Optimized â†’ Enhanced â†’ Legacy)
- Page title management

**ğŸ’¾ Database Operations (`db_operations.py`)**
- `DatabaseOperations` class with all DB functionality
- Connection management and metadata tracking
- Page saving with intelligent content splitting

## ğŸŒ Wiki Crawler Features

### Content Extraction
- **Complete fandom-py independence** - Uses only MediaWiki API
- **Multiple extraction strategies** with intelligent fallbacks
- **Full mission log support** - Extracts 100k+ character mission logs
- **Optimized performance** - Average 0.36 seconds per page

### Content Processing
- **Intelligent classification** - Mission logs, ship info, personnel, locations
- **Ship name detection** - Supports 22nd Mobile Daedalus Fleet ships
- **Content formatting** - Structured markdown with sections and metadata
- **Large content handling** - Automatic splitting for database storage

### Database Integration
- **PostgreSQL integration** - Direct storage to elsiebrain database
- **Metadata tracking** - Page versions, crawl history, error tracking
- **Schema compatibility** - Both `content` and `raw_content` fields populated
- **Change detection** - Skip unchanged pages for efficiency

## ğŸ“‹ Usage

### Wiki Crawler Commands

```bash
# Standard crawl (~26 curated pages)
python wiki_crawler.py

# Comprehensive crawl (all pages - WARNING: Very slow!)
python wiki_crawler.py --comprehensive

# Force update all pages (ignore change detection)
python wiki_crawler.py --force

# Show database statistics
python wiki_crawler.py --stats

# Crawl specific page
python wiki_crawler.py "PAGE_TITLE"

# Show help and architecture info
python wiki_crawler.py --help
```

### Performance Metrics
- **Success Rate**: 65%+ (improved from 50%)
- **Processing Speed**: 0.36 seconds per page average
- **Mission Log Extraction**: Full 100k+ character support
- **API Efficiency**: Combined calls reduce requests by ~60%

### Database Backup Commands

```bash
# Create a timestamped backup
python backup_db.py backup

# Create/update seed backup
python backup_db.py seed

# Restore from seed backup
python backup_db.py restore

# Restore from specific backup
python backup_db.py restore path/to/backup.sql
```

## ğŸ“Š Database Schema

The crawler populates these key tables:

- **`wiki_pages`** - Main content storage with classification
- **`page_metadata`** - Crawl history and change tracking

Content is automatically classified as:
- `mission_log` - Ship mission logs with date and ship extraction
- `ship_info` - USS ship information pages
- `personnel` - Character and crew information
- `location` - Planets, systems, stations
- `general` - Other wiki content

## ğŸ”§ Technical Features

### Performance Optimizations
- **Combined API calls** - Single request for multiple data types
- **Intelligent content routing** - Fast path for high-quality extracts
- **Retry logic** - Automatic failure recovery
- **Change detection** - Skip unchanged content

### Content Quality
- **Multi-strategy extraction** - Optimized â†’ Enhanced â†’ Legacy â†’ Wikitext
- **Fallback mechanisms** - Robust handling of edge cases
- **Content validation** - Minimum thresholds with flexible acceptance
- **Error classification** - Detailed error reporting and tracking

## ğŸ³ Docker Integration

The system is designed to run within the db_populator Docker container:

```bash
# Build and run the container
docker-compose up db_populator

# Or run specific commands
docker exec db_populator python wiki_crawler.py --stats
```

## ğŸ“ Backup Files

- `backups/seed_backup.sql`: Base database state (tracked in git)
- `backups/backup_YYYYMMDD_HHMMSS.sql`: Timestamped backups (not tracked in git)

## âš™ï¸ Environment Variables

Required variables in `.env`:
```
DB_NAME=elsiebrain          # Database name
DB_USER=elsie               # Database user  
DB_PASSWORD=elsie123        # Database password
DB_HOST=localhost           # Database host
DB_PORT=5433                # Database port (5433 for development)
```

## ğŸ¯ Development Benefits

### Maintainability
- **Focused modules** - Single responsibility principle
- **Easy debugging** - Issues isolated to specific components
- **Clear dependencies** - Explicit module relationships

### Extensibility  
- **Plugin architecture** - Easy to add new extraction strategies
- **Modular API client** - Simple to add new MediaWiki operations
- **Flexible processing** - Easy to add new content classification types

### Testing
- **Unit testable** - Individual components can be tested in isolation
- **Mock-friendly** - Clean interfaces for testing
- **Reduced complexity** - Smaller, focused test suites

## ğŸ“ˆ Migration from Legacy

The new modular architecture maintains **100% backward compatibility** while providing:
- **83% reduction** in main file size (253 vs 1280+ lines)
- **Complete fandom-py removal** - No external dependencies
- **Enhanced performance** - Faster and more reliable
- **Better error handling** - Improved reliability and debugging

## ğŸ†˜ Troubleshooting

### Common Issues

1. **Import Errors**: Ensure all module files are in the same directory
2. **Database Connection**: Check `.env` file configuration
3. **API Rate Limits**: The crawler includes automatic rate limiting
4. **Memory Issues**: Large mission logs are automatically split

### Debug Commands
```bash
# Test specific page extraction
python wiki_crawler.py "Main Page"

# Check database connectivity
python wiki_crawler.py --stats

# Verify module imports
python -c "from wiki_crawler import WikiCrawlerContainer; print('âœ“ Modules OK')"
```

---

**Version**: Modular Architecture v2.0  
**Last Updated**: 2025-06-11  
**Compatibility**: Python 3.7+, PostgreSQL 12+ 